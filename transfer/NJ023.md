# FedCKD: KD-based Classifier Refinement for Non-IID Data in Federated Learning  

Chao Xu School of Computer Science, Nanjing Audit University, Nanjing, China xuchao@nau.edu.cn  

Wenzhang Su School of Computer Science, Nanjing Audit University, Nanjing, China mp2309041@stu.nau.edu.cn  

Fanfan Shen School of Computer Science, Nanjing Audit University, Nanjing, China ffshen@whu.edu.cn  

# Jun Zhang  

Yanxiang He School of Computer Science, Wuhan University,Wuhan yxhe@whu.edu.cn  

# Yong Chen  

School of Software, East China University of Science and Technology, Nanchang 664633281@qq.com  

School of Computer Science, Nanjing Audit University, Nanjing, China chenyong@nau.edu.cn  

Abstract —In federated learning, client data typically exhibits non-independent and identically distributed (non-IID) character- istics, which introduce classifier bias across clients and degrade global model performance. To tackle this challenge, we propose FedCKD (Federated Classifier Refinement via Knowledge Distil- lation), a framework that mitigates data heterogeneity by refining the global classifier on the server through knowledge distillation. Specifically, each client fits its local feature distribution with a multi-component Gaussian Mixture Model (GMM) and uploads the parameters to the server. The server then generates virtual feature samples from the aggregated statistics, performs weighted aggregation of soft labels to construct teacher vectors, and finally applies a decoupled knowledge distillation mechanism to refine the global classifier. The experimental results show that under the most severe data heterogeneity setting on the CIFAR-10 dataset, FedCKD outperforms state-of-the-art algorithms by   ${\bf6.93\%}$   and  $\mathbf{1.09\%}$   in accuracy. Moreover, it also outperforms other baseline methods under non-IID settings across different datasets.  

CCVR (Classifier Calibration with Virtual Representations) [6] identifies the classifier layer bias as a primary reason for performance degradation in federated learning with non- IID data. It highlights the low feature similarity among local classifiers across clients, along with weight norms that dis- proportionately favor majority classes. Consequently, classi- fier optimization becomes essential for improving federated learning performance under data heterogeneity. Knowledge distillation [7], as a model compression and transfer learn- ing technique, has recently been widely adopted to miti- gate data heterogeneity in FL. Based on the findings of CCVR, we propose FedCKD (Federated Classifier Refinement via Knowledge Distillation). Unlike CCVR, which performs post hoc calibration using single-Gaussian virtual features for each class, FedCKD continuously refines the classifier during training. It captures aggregated statistical knowledge via a multi-component Gaussian Mixture Model (GMM), and applies class-aware knowledge distillation guided by client label distributions.  

Index Terms —Federated Learning, Non-IID Data, Knowledge Distillation, Classifier Refinement  

# I. I NTRODUCTION  

Federated learning (FL) is an emerging distributed machine learning paradigm that enables clients to collaboratively train models while maintaining data privacy [1] [2]. However, data across clients in FL is typically non-independent and identically distributed (non-IID), exhibiting imbalanced sam- ple sizes, heterogeneous feature spaces and skewed label distributions. To tackle this issue, numerous studies have been conducted. Scaffold (Stochastic Controlled Averaging for Federated Learning) [3] mitigates client drift caused by data heterogeneity, using control variates on the server to correct the direction of client updates. FedNova (Federated Normalized Averaging) [4] enables flexible hyperparameter selection on the server and promotes consistent global model convergence by reweighting client contributions during ag- gregation. Instead of traditional model averaging, FedConcat (Federated Learning with Model Concatenation) [5] adopts a model concatenation framework that clusters clients, trains feature extractors within each cluster, and jointly trains a global classifier to mitigate the impact of data heterogeneity.  

Specifically, our main contributions are as follows:  

We propose a virtual feature generation method in which • each client models its local features using a multi- component GMM, and the server aggregates them in two stages to construct a global per-class GMM. Based on this, class-balanced synthetic features are generated to calibrate the global classifier.  We design a class-aware teacher vector by weighting • client soft logits with label distributions, and refine the global classifier using a decoupled distillation loss that separately supervises target and non-target classes to mitigate inter-client bias.  We conduct extensive experiments on the CIFAR-10, • CIFAR-100, CINIC-10 and MNIST datasets under vary- ing degrees of data heterogeneity. The results show that FedCKD consistently outperforms baseline methods and exhibits strong generalization ability, especially under severe non-IID settings on the CIFAR-10.  

![](images/cba22059c0aaec394c9de9722a431831bb5d8bfc5c3691d98d6ae124a6df43eb.jpg)  
Figure 1: The framework of FedCKD. In FedCKD, clients not only update their local model parameters to    $w_{k}^{(t+1)}$  , but also collect extra statistical knowledge to upload to the server. The server leverages this knowledge to perform decoupled knowledge distillation training on the global classifier. (The elements in the framework are obtained from iconfont.)  

# II. PROPOSED METHOD  

# A. Federated Learning Setup  

We consider a federated learning system consisting of    $N$  clients and a central server. Each client  $k$   holds a local dataset  $\mathcal D_{k}\ =\ (x_{i},y_{i})_{i=1}^{|\mathcal D_{k}|}$    , where    $x_{i}$   is the input data,    $y_{i}$   is the corresponding label and    $\lvert\mathcal{D}_{k}\rvert$   is the number  f samples in the local dataset. In each communication round  t , the server ran- domly selects  $\mathcal{K}_{t}$   clients to participate in collaborative training. The selected clients receive the global model parameters  $w_{g}^{(\check{t})}$  broadcast by the server and perform    $E_{k}$   local training epochs. Each client updates the local model parameters to  $w_{k}^{(t\preceq_{1})}$  based on its local data and uploads it to the server for aggregation. The objective of   $\mathrm{FL}$   can be expressed as in [1]:  

$$
\operatorname*{min}_{w_{g}^{(t)}}\sum_{k\in\mathcal{K}_{t}}\frac{\left|\mathcal{D}_{k}\right|}{\sum_{j\in\mathcal{K}_{t}}\left|\mathcal{D}_{j}\right|}\cdot\mathcal{L}_{k}\left(w_{g}^{(t)}\right)
$$  

where  $\mathcal{L}_{k}$   is the local loss function of client  $k$  

Since the classifier is directly influenced by the local label distribution, classifiers on different clients can exhibit significant bias and inconsistency in non-IID scenarios [6]. In addition to standard training, clients are required to provide extra knowledge to assist in refining the classifier. The Fig. 1 illustrates the design framework of the FedCKD algorithm. A detailed implementation is presented in the following section.  

B. Client Training  

For each participating client    $k$  , in addition to performing standard local training to update its local model to    $\overset{\cdot}{w}_{k}^{(t+1)}$  , it is also required to collect extra local knowledge after each local training phase. This knowledge includes feature distribution statistics and the statistical information of classifier outputs.  

We assume that the entire dataset consists of    $C$   classes. For each sample in class    $c$     $(c\,\in\,C)$  , the local model extracts a feature vector  $h_{k,c}^{(s)}$  , where    $s=1,2,\ldots,n_{k,c}$   and    $n_{k,c}$   denotes the number of samples  n class    $c$   on client  $k$  . If    $n_{k,c}\ge\gamma\;(\gamma>$  1 ), meaning that class  c  has sufficient samples, we model its feature distribution using a multi-component GMM as follows:  

$$
p(h\mid c)=\sum_{j=1}^{M_{k,c}}\pi_{k,c}^{(j)}\mathcal{N}(h;\mu_{k,c}^{(j)},\mathrm{diag}(\sigma_{k,c}^{2(j)}))
$$  

where    $\mathcal{N}()$   denotes a Gaussian distribution,    $M_{k,c}$   is the num- ber of Gaussian components used for class    $c$  ,    $\pi_{k,c}^{(j)}$    represents e weight of the  $j$  -th com  $(\textstyle\sum_{j}\pi_{k,c}^{(j)}=1)$  ). The  $\mu_{k,c}^{(j)}\in\mathbb{R}^{D}$  vector of the    is  $j$  he mean vector and -th component.  $\sigma_{k,c}^{2(j)}\in\mathbb{R}^{D}$     is the variance  

When    $n_{k,c}~<~\gamma$  , the number of samples in class    $c$   is insufficient to reliably fit a multi-component GMM. In this case, a single Gaussian distribution is used to model the class, and the sample mean and variance for each feature dimension  $d$   $(d=1,2,.\,.\,.\,,D)$   are computed as follows:  

$$
\mu_{k,c,d}=\frac{1}{n_{k,c}}\sum_{s=1}^{n_{k,c}}h_{k,c,d}^{(s)}
$$  

$$
\sigma_{k,c,d}^{2}=\frac{1}{n_{k,c}}\sum_{s=1}^{n_{k,c}}\left(h_{k,c,d}^{(s)}-\mu_{k,c,d}\right)^{2}
$$  

To support the construction of teacher vectors on the server, local model is also required to upload soft label knowledge from the classifier outputs. Specifically, we first compute the average logits for class    $c$   as follows:  

$$
\bar{z}_{k,c}=\frac{1}{n_{k,c}}\sum_{s=1}^{n_{k,c}}z_{k,c}^{(s)}
$$  

where    $z_{k,c}$   denotes the classifier output of client  $k$   for class    $c$  . To further capture the uncertainty in the outputs, the variance of the logits for each class is also uploaded:  

$$
\mathrm{var}(z_{k,c})=\frac{1}{n_{k,c}}\sum_{s=1}^{n_{k,c}}\left(z_{k,c}^{(s)}-\bar{z}_{k,c}\right)^{2}
$$  

# C. Server Aggregation and Refinement  

Upon receiving the additional statistical knowledge up- loaded by clients, the server aggregates it to construct teacher vectors and a virtual feature dataset. To generate virtual fea- tures for each class    $c$  , we perform a two-stage aggregation to preserve the mixture structure. Firstly, if the GMMs uploaded by clients for the class    $c$   have the same number of components, we align them by sorting local GMM components, and use the client sample sizes as weights to obtain the global statistics:  

$$
\tilde{\mu}_{c,j,d}=\frac{\sum_{k\in\mathcal{K}_{t}}n_{k,c}\mu_{k,c,d}^{(j)}}{\sum_{k\in\mathcal{K}_{t}}n_{k,c}}
$$  

$$
\tilde{\sigma}_{c,j,d}^{2}=\frac{\sum_{k\in\mathcal{K}_{t}}n_{k,c}\left(\sigma_{k,c,d}^{2(j)}+(\mu_{k,c,d}^{(j)}-\tilde{\mu}_{c,j,d})^{2}\right)}{\sum_{k\in\mathcal{K}_{t}}n_{k,c}}
$$  

$$
\tilde{\pi}_{c,j}=\frac{\sum_{k\in\mathcal{K}_{t}}n_{k,c}\pi_{k,c}^{(j)}}{\sum_{k\in\mathcal{K}_{t}}n_{k,c}}
$$  

where    $\tilde{\mu}_{c,j,d},\;\tilde{\sigma}_{c,j,d}^{2}$    and  $\tilde{\pi}_{c,j}$   are the mean, diagonal variance and mixture weight of the    $j$  -th   $(j=1,.\,.\,.\,,M)$   component in the aggregated distribution for class    $c$  .  

If the same class appears with different component counts among clients, we first perform the aggregation steps (7)- (9) within each component-count group separately, obtaining distinct aggregated sub-GMMs. Next, we concatenate these sub-GMMs to form a unified multi-component GMM. To properly handle the differences in component counts, we scale the mixture weights of each sub-GMM according to the total number of samples from the corresponding client groups. After concatenation, the mixture weights are re-normalized to en ure that    $\begin{array}{r}{\sum_{\ell=1}^{L_{c}}\tilde{\pi}_{c,\ell}\,=\,1\ }\end{array}$  , where    $L_{c}$   is the total number of components kept for class    $c$  . The global distribution for class  c  is then represented by a unified multi-component GMM defined as:  

$$
\tilde{p}(h\mid c)=\sum_{\ell=1}^{L_{c}}\tilde{\pi}_{c,\ell}\mathcal{N}\big(h;\tilde{\mu}_{c,\ell},\mathrm{diag}(\tilde{\sigma}_{c,\ell}^{2})\big)
$$  

If all clients upload single-component Gaussian statistics for a given class, the server aggregates them by comput- ing the standard mean and variance weighted by the client sample sizes    $n_{k,c}$  . To generate virtual features for class  $c$  , we first sample a component index    $\ell~\sim~\mathrm{Cat}(\tilde{\pi}_{c,1:L_{c}})$  

(categorical distribution), and then sample a feature vector  $x~\sim~\mathcal{N}(\tilde{\mu}_{c,\ell},\mathrm{diag}(\tilde{\sigma}_{c,\ell}^{2}))$  . Assuming independence between feature dimensions, the density of the sampled virtual feature  $x$   is expressed as:  

$$
p(x\mid c,\ell)=\prod_{d=1}^{D}{\frac{1}{\sqrt{2\pi\tilde{\sigma}_{c,\ell,d}^{2}}}}\exp[-{\frac{(x_{d}-\tilde{\mu}_{c,\ell,d})^{2}}{2\tilde{\sigma}_{c,\ell,d}^{2}}}]
$$  

The server randomly generates    $N^{*}$   $({N^{*}}\mathrm{~\ensuremath~{~>~}~}0)$  ) virtual features for each class based on the predefined number of virtual samples. To construct a class-aware teacher, we first weighted fusion, and the aggregated mean and variance are utilize the class-aware weights    $\begin{array}{r}{\alpha_{k,c}=\frac{n_{k,c}}{\sum_{k\in\mathcal{K}_{t}}n_{k,c}}}\end{array}$  P  to perform obtained as follows:  

$$
\begin{array}{r l}&{\quad\bar{z}_{g,c}=\displaystyle\sum_{k\in\mathcal{K}_{t}}\alpha_{k,c}\,\bar{z}_{k,c},}\\ &{\mathrm{var}(z_{g,c})=\displaystyle\sum_{k\in\mathcal{K}_{t}}\alpha_{k,c}\big[\mathrm{var}(z_{k,c})+(\bar{z}_{k,c}-\bar{z}_{g,c})^{2}\big]}\end{array}
$$  

Accordingly, the aggregated per-class logit is modeled as:  

$$
Z_{g,c}=\mathcal{N}\left(\bar{z}_{g,c},\mathrm{var}(z_{g,c})\right)
$$  

The server performs multi-view sampling for each class    $c$  by drawing    $\theta$     $(\theta\ >\ 1)$  ) samples    $z_{g,c}^{(v)}\ \stackrel{\cdot}{\sim}\ \breve{Z}_{g,c}\,(v=1,.\,.\,.\,,\theta)$    ∼ from the corresponding normal distribution  N . Each sampled value is then processed with temperature scaling followed by a softmax operation. The final teacher vector  $p_{t}$   is defined as:  

$$
p_{t,i}=\frac{1}{\theta}\sum_{v=1}^{\theta}\frac{\exp\left(z_{g,i}^{(v)}/\tau\right)}{\sum_{j=1}^{C}\exp\left(z_{g,j}^{(v)}/\tau\right)},\quad i=1,\dots,C.
$$  

where  $\tau$   is the temperature parameter.  

During the knowledge distillation process, the teacher vector is decoupled into target and non-target components. For each virtual sample    $x$  , the server uses the current global model’s classifier as the student model to perform a forward pass and obtain the output    $z_{s}(x)$  . When supervising the target class component, the distribution of the student model is given by:  

$$
p_{s,i}=\frac{\exp\left(z_{s,i}(x)\right)}{\sum_{j=1}^{C}\exp\left(z_{s,j}(x)\right)},\quad i=1,\ldots,C.
$$  

For the target class label    $y$  , the corresponding distribution is denoted as  $p_{s}(y)$  . As it primarily reflects hard label super- vision, the target loss is computed via cross-entropy:  

$$
\mathcal{L}_{C E}=-\log\left(p_{s}(y)\right)
$$  

For the non-target class, in order to transfer the dark knowl- edge that reflects class-level distinctions in a fine-grained manner, temperature scaling is applied to the output of the student model. The resulting distribution of the student model is given by:  

$$
p_{s,i}^{\prime}=\frac{\exp{(z_{s,i}(x)/\tau)}}{\sum_{j=1}^{C}\exp{(z_{s,j}(x)/\tau)}},\quad i\neq y
$$  

![TABLE I: Comparison of Top-1 accuracy of various algorithms under non-IID data ](images/4013147cbd3115835092da7c7ccbe98a1a1fb688e31d5831aa506139a58f7d54.jpg)  

Accordingly, Kullback-Leibler (KL) divergence is used to align the outputs of the teacher and student models:  

$$
\mathcal{L}_{K D}=\tau^{2}\sum_{i\neq y}p_{t,i}\log\frac{p_{t,i}}{p_{s,i}^{\prime}}
$$  

The overall loss for distillation on a virtual sample    $x$   is then computed as the weighted sum of the target and non-target components:  

$$
\mathcal{L}_{\mathrm{total}}=\mathcal{L}_{\mathrm{CE}}+\beta\cdot\mathcal{L}_{\mathrm{KD}}
$$  

where  $\beta$   is used to balance the two components of the loss.  

The server uses the virtual feature dataset to supervise the global classifier, optimizing it via multiple epochs of gradient descent guided by the total distillation loss. After    $E_{g}$   epochs of training, the distilled and refined global feature classifier is obtained and broadcast to the clients to initiate the next round of federated training.  

# III. EXPERIMENTS  

# A. Experimental Setup  

Dataset.  Our experiments are conducted on four datasets: CIFAR-10, CIFAR-100, CINIC-10 and MNIST, with CIFAR- 10 serving as the primary dataset. We adopt the Latent Dirich- let Allocation (LDA) strategy to simulate data heterogeneity among clients, where the parameter    $\alpha$   controls the degree of heterogeneity. As  $\alpha$   decreases, the degree of data heterogeneity increases. On the CIFAR-10 dataset, we set    $\alpha$   to 0.05, 0.1, 0.3 and 0.5. For the other datasets,    $\alpha$   is fixed at 0.1. Our experiments are implemented in Python 3.8 using the PyTorch framework (version 1.10.0 with CUDA 11.3), and executed on a machine equipped with an NVIDIA RTX 3090 GPU and Intel(R) Xeon(R) Gold 6330 CPU   $@~2.00\mathrm{GHz}$  .  

Baseline.  FedCKD is fairly compared against five baselines: FedAvg [1], FGA [2], Scaffold [3], FedConcat [5] and CCVR [6]. For all algorithms, we use a lightweight CNN model consisting of two convolutional and pooling layers. After feature extraction, the classifier includes a fully connected layer that maps the 512-dimensional feature vector to the number of classes, with ReLU activation and dropout applied.  

Implementation.  For the federated learning task, we set the total number of clients to    $N=100$  , and randomly select  $\mathcal{K}_{t}=10$   clients to p te in training during each round. Each client performs  $E_{k}=5$   local training epochs based on its own data, and the total number of communication rounds is set to    $T\:=\:200$  . Specifically, for CCVR and FedCKD, the number of additional server-side refinement steps is set to  $E_{g}=1$  , and the number of virtual samples generated per class is    $N^{*}\,=\,200$  . For all methods, we use Stochastic Gradient Descent (SGD) to optimize the classifier, with a learning rate of 0.01, momentum of 0.9 and weight decay of 1e-5. Hyperparameters for all methods are carefully selected through grid search to achieve optimal results across all experiments.  

# B. Experimental Results and Analysis  

Tab. I presents the performance of various algorithms un- der different settings across four datasets, using Top-1 test accuracy as the evaluation metric. On the CIFAR-10 dataset with the most severe non-IID setting (  $\alpha\,=\,0.05)$  ), FedCKD outperforms CCVR and FedConcat with accuracy improve- ments of  $6.93\%$   and   $1.09\%$  , respectively. Notably, the Scaffold fails to converge when    $\alpha=0.05$  . As    $\alpha$   increases, all methods show a consistent rise in accuracy, with FedCKD maintaining a leading position throughout. Notably, when    $\alpha\,=\,0.5$   and data heterogeneity is no longer the primary bottleneck, the FedConcat algorithm achieves the highest accuracy. However, FedCKD still maintains strong competitiveness and is on a par with the best-performing baseline.  

The experimental results under varying degrees of data heterogeneity show that FedCKD performs best with moderate to high heterogeneity   $(\alpha~\leq~0.3)$  . Its effectiveness stems from the ability to generate complementary virtual features for minority classes and construct multi-view teacher vectors. A decoupled distillation loss further fine-tunes the global classifier by aligning hard-label boundaries with nuanced dark knowledge. FedCKD mitigates weight inflation in majority classes while enhancing minority class discrimination, without causing negative transfer as heterogeneity decreases.  

Meanwhile, FedCKD achieves the best results on the CIFAR-100 and CINIC-10 datasets   $(\alpha=0.1)$  ), outperforming CCVR by   $7.06\%$   and   $1.75\%$  , respectively. However, on the simpler MNIST handwritten digit dataset   $(\alpha=0.1)$  ), FedCKD ranks second but still remains competitive with the best- performing baseline.  

# C. Discussion  

To further support the experimental analysis, we investigate three key hyperparameters: the number of local training epochs on the client side  $E_{k}$  , the number of refinement steps on the server side    $E_{g}$   and the number of virtual samples generated per class    $N^{*}$  . All experiments in this section are conducted on the CIFAR-10 dataset with  $\alpha=0.05$  .  

We evaluate    $E_{k}$   values of 1, 5, 10 and 20, with results shown in Fig. 2. As  $E_{k}$   increases, the accuracy of all methods consistently improves. This is attributed to more sufficient local gradient updates and reduced communication frequency. When    $E_{k}~=~20$  , the performance gap between algorithms narrows significantly, yet FedCKD continues to outperform all baselines. The results demonstrate that FedCKD is robust to the number of local training epochs. However, a higher  $E_{k}$   also brings significant computational overhead per communication round for all methods.  

![](images/83afed9ba2452665568faf78275c24e3b8ec323530267ccc67975f8c8dfd7fea.jpg)  
Figure 2: Accuracy comparison under different local training epochs    $E_{k}$  

Since both FedCKD and CCVR refine the classifier on the server side, we compare them by varying the number of server-side training epochs    $E_{g}\,\in\,\{1,2,3,5\}$  . The results are shown in Tab. II. It can be observed that the accuracy of both methods improves with an increasing number of refinement steps. For CCVR, the gain becomes marginal at    $E_{g}~=~3$  , and performance even drops at    $E_{g}\,=\,5$  . This is because the training dataset is generated from sampled features, which introduces some noise. Increasing the number of training epochs excessively amplifies the impact of this noise in the dataset. With the aid of knowledge distillation, FedCKD does not experience a significant accuracy decline at    $E_{g}~=~5$  , effectively mitigating the negative impact of noise.  

![TABLE II: Accuracy comparison under different server-side training epochs    $E_{g}$  ](images/331badc34a75b4bffcd22f2a16dc90427129dbb6f904e4631e3b7866edd5b876.jpg)  

![TABLE III: Comparison of accuracy under varying virtual sample sizes    $N^{*}$  ](images/f24b59e891647aa4f6bf45d161d3c989da8bc0d9ebf5729147842e2a5a9ae575.jpg)  

Another key factor we examine is the number of virtual features generated, as shown in Tab. III. As the number of generated virtual features increases, the accuracy of CCVR and FedCKD improves. When the number of virtual features generated by FedCKD is    $N^{*}\,=\,50$  , its accuracy approaches that of CCVR with  $N^{*}=200$  . This demonstrates that knowl- edge distillation for refinement can achieve better accuracy with fewer virtual features.  

# IV. CONCLUSION  

In this paper, we propose FedCKD, a federated learning framework designed to address data heterogeneity by refining the global classifier on the server side via knowledge distilla- tion. FedCKD models local feature distributions using a multi- component Gaussian Mixture Model and constructs class- aware teacher vectors on the server. By decoupling knowledge distillation, it effectively calibrates classifier bias. Experi- mental results show that FedCKD consistently outperforms prior methods in both accuracy and robustness. Moreover, by generating virtual features instead of transmitting raw data, it effectively preserves data privacy. In future work, we aim to scale FedCKD to broader deployment scenarios and further reduce communication overhead.  

# A CKNOWLEDGMENT  

This work has been supported by the Basic Science (Natural Science) Research Project of Colleges and Universities in Jiangsu Province (22KJA520004, 24KJA520005), the National Natural Science Foundation of China (61902189, 62472227, 62162002, 61972293), Postgraduate Research  &  Practice In- novation Program of Jiangsu Province (KYCX25 2479).  

# R EFERENCES  

[1] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efficient learning of deep networks from decentralized data,” in  Artificial intelligence and statistics . PMLR, 2017, pp. 1273– 1282.

 [2] S. W. Remedios, J. A. Butman, B. A. Landman, and D. L. Pham, “Federated gradient averaging for multi-site training with momentum- based optimizers,” in  Domain Adaptation and Representation Transfer, and Distributed and Collaborative Learning . Springer, 2020, pp. 170– 180.

 [3] S. P. Karimireddy, S. Kale, M. Mohri, S. Reddi, S. Stich, and A. T. Suresh, “Scaffold: Stochastic controlled averaging for federated learning,” in  International conference on machine learning . PMLR, 2020, pp. 5132–5143.

 [4] J. Wang, Q. Liu, H. Liang, G. Joshi, and H. V. Poor, “Tackling the ob- jective inconsistency problem in heterogeneous federated optimization,” Advances in neural information processing systems , vol. 33, pp. 7611– 7623, 2020.

 [5] Y. Diao, Q. Li, and B. He, “Exploiting label skews in federated learning with model concatenation,” in  Proceedings of the AAAI Conference on Artificial Intelligence , vol. 38, no. 10, 2024, pp. 11 784–11 792.

 [6] M. Luo, F. Chen, D. Hu, Y. Zhang, J. Liang, and J. Feng, “No fear of heterogeneity: Classifier calibration for federated learning with non-iid data,”  Advances in Neural Information Processing Systems , vol. 34, pp. 5972–5984, 2021.

 [7] B. Zhao, Q. Cui, R. Song, Y. Qiu, and J. Liang, “Decoupled knowledge distillation,” in  Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition , 2022, pp. 11 953–11 962.  